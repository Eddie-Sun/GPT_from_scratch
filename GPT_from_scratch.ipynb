{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_i2sFYqTD4S",
        "outputId": "952673fb-c586-46e4-d648-53fc684561db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-22 02:37:59--  https://sherlock-holm.es/stories/plain-text/cano.txt\n",
            "Resolving sherlock-holm.es (sherlock-holm.es)... 157.90.249.21, 2a01:4f8:1c17:5725::1\n",
            "Connecting to sherlock-holm.es (sherlock-holm.es)|157.90.249.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3868223 (3.7M) [text/plain]\n",
            "Saving to: ‘cano.txt’\n",
            "\n",
            "cano.txt            100%[===================>]   3.69M  5.40MB/s    in 0.7s    \n",
            "\n",
            "2023-12-22 02:38:00 (5.40 MB/s) - ‘cano.txt’ saved [3868223/3868223]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Training data: sherlock holmes - all stories\n",
        "!wget https://sherlock-holm.es/stories/plain-text/cano.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read + inspect\n",
        "with open('cano.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "print(\"length of file in char: \", len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cekHcvB7YvoD",
        "outputId": "5edcc0a0-5d8f-454a-ca52-547b3db08a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of file in char:  3868122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, corpus):\n",
        "        self.vocab = self.build_vocab(corpus)\n",
        "        self.inverse_vocab = {index: token for token, index in self.vocab.items()}\n",
        "\n",
        "    def build_vocab(self, corpus):\n",
        "        # Tokenize the corpus at the desired level\n",
        "        tokens = corpus.split()\n",
        "        # Manually count the frequency of each token\n",
        "        token_freq = {}\n",
        "        for token in tokens:\n",
        "            if token in token_freq:\n",
        "                token_freq[token] += 1\n",
        "            else:\n",
        "                token_freq[token] = 1\n",
        "        # Sort tokens by frequency in descending order and then alphabetically\n",
        "        sorted_tokens = sorted(token_freq.items(), key=lambda item: (-item[1], item[0]))\n",
        "        # Create a vocab dictionary mapping tokens to unique indices\n",
        "        vocab = {token: index for index, (token, _) in enumerate(sorted_tokens)}\n",
        "        return vocab\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Tokenize the text at the desired level\n",
        "        tokens = text.split()\n",
        "        # Convert tokens to indices based on the vocabulary\n",
        "        return [self.vocab.get(token, self.vocab.get('[UNK]', -1)) for token in tokens]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        # Convert a list of indices back into text\n",
        "        return ' '.join(self.inverse_vocab.get(index, '[UNK]') for index in indices)\n",
        "\n",
        "# Example usage:\n",
        "corpus = \"This is an example corpus for building a tokenizer. It may contain example sentences.\"\n",
        "tokenizer = Tokenizer(corpus)\n",
        "\n",
        "encoded = tokenizer.encode(\"This is a test sentence.\")\n",
        "print(encoded)\n",
        "\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh_nZKSvbIdT",
        "outputId": "0492429a-d5c7-4fae-b351-8a5b49fef1cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 9, 3, -1, -1]\n",
            "This is a [UNK] [UNK]\n"
          ]
        }
      ]
    }
  ]
}